{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-based reinforcement learning\n",
    "Ben Mann and Andrew Schreiber\n",
    "### Motivation\n",
    "\n",
    "Model-based learning \n",
    "\n",
    "- may be more introspectable than model-free learning. \n",
    "- is outperforming model-free learning at irreversible games (see [Sokoban imagination](https://arxiv.org/pdf/1707.06203.pdf)). It hasn't successfully been applied to Atari. \n",
    "- algorithms can be informed that part of the training space is untouchable (i.e. donâ€™t simulate human minds being tortured, we can tell you ahead of time that is bad). \n",
    "\n",
    "### Summary of idea\n",
    "\n",
    "DQN learns on CartPole much faster when input is [velocity, position, angle, angular velocity] vs pixels.\n",
    "\n",
    "Can we use an unsupervised generative model to collapse a high dimensional state representation to speed up model-free learning? Evaluate on classic control problems and basic Atari games.\n",
    "\n",
    "### Related work\n",
    "\n",
    "[5] uses a deep VAE to learn a low-dimensional representation of classic control problems, but doesn't use reinforcement learning on top of it.\n",
    "\n",
    "[7] and [9] build on Atari next video frame prediction [6] but fail to beat DQN performance.\n",
    "\n",
    "[8] merges model-based and model-free techniques but doesn't report Atari success.\n",
    "\n",
    "[2] uses the idea of combining model-based and model-free techniques for the purpose of data efficiency, but it doesn't operate on pixels, and in mujoco state + action should perfectly predict next state unlike atari where other agents can do stuff\n",
    "\n",
    "  \n",
    "\n",
    "So the big difference here is that we're aiming to beat DQN at data efficiency on Atari using an approximation of f(s, a) -&gt; s'. We start with CartPole to validate the approach and move on to Pong.\n",
    "\n",
    "### Project Roadmap / ideas\n",
    "\n",
    "1. [Cartpole](https://arxiv.org/pdf/1602.01783v1.pdf), [MountainCar](https://gym.openai.com/envs/MountainCar-v0), [Pendulum](https://gym.openai.com/envs/Pendulum-v0) \n",
    "    1. A2C on dense representation learns fast (upper bound of learning speed). Compare to \n",
    "        1. A2C trained on autoencoder-learned dense representation \n",
    "        2. A2C trained on pixels directly \n",
    "\n",
    "2. Pong \n",
    "    1. Is a dense representation learned from full-resolution color better than from downsample? \n",
    "        1. How much does model capacity matter as input resolution varies? \n",
    "\n",
    "    2. Can we learn faster than from pixels? \n",
    "    3. [This Stanford group failed to beat DQN](http://cs231n.stanford.edu/reports/2016/pdfs/116_Report.pdf) [9]. They say it's because compounding errors made long-term prediction hard, and pong and breakout have very long (>50) timestep dependencies. Seems like we could get around this by \n",
    "        1. predicting N steps ahead in one forward pass and\n",
    "        2. feed all N steps into model a la I2A [8] so that later frames can be discarded dynamically.\n",
    "    4. Can we do long-range, cheap rollouts using [dilated convolutions](http://www.inference.vc/dilated-convolutions-and-kronecker-factorisation/) [11]? Sokoban noted even 5-step rollouts were quite expensive \n",
    "\n",
    "3. Breakout(?) \n",
    "    1. Might be easier than pong due to denser rewards \n",
    "\n",
    "4. Montezuma (stretch goal) \n",
    "    1. Try adding noise to the input image, train an Inspector \n",
    "    2. Detect abnormally high variance in autoencoder's decoded output frame, use that as a signal to update model \n",
    "    3. Train new autoencoder per room, auto-detect when out of distribution a la \"Modular Multitask Reinforcement Learning with Policy Sketches\" [https://arxiv.org/abs/1611.01796v2](https://arxiv.org/abs/1611.01796v2) \n",
    "    4. Use an ensemble of autoencoders so that any of them can be swapped out or modified and agent still performs \n",
    "\n",
    "### Concrete steps\n",
    "\n",
    "1. Run env (CartPole-v0, Pendulum, MountainCar) \n",
    "2. Generate training data for autoencoder model using pre-trained DQN / random agent interpolation to maximize search space \n",
    "3. Train autoencoder to repro -- a/b split here \n",
    "    1. a frame \n",
    "    2. Sequence of frames \n",
    "\n",
    "4. Use dense autoencoder representation to preprocess frames and train a model \n",
    "5. Compare to upper and lower bound baselines (A2C trained on \"classic control\" signals and A2C trained on pixels) \n",
    "\n",
    "### Additional questions\n",
    "\n",
    "1. Does the learned autoencoder representation correlate with the real 4 dimensional cartpole observation? Chart r^2 for all combinations of variables. Chart scatter x vs y. \n",
    "2. For a random input state, what is the delta in the decoded representation given one action vs the other? If it's small, can we add a term to the loss to make it larger? \n",
    "3. How do loss and computational cost differ given different initial downsampling? Can we get away with preserving more detail? Does using 2 frame stack instead of 4 help? \n",
    "4. How quickly do imagination rollouts drift? How bad do they look? \n",
    "5. Would hard attention be an effective dimensionality reducer since most of the space is blank? \n",
    "6. How much better is VAE vs vanilla AE? \n",
    "  \n",
    "\n",
    "### Literature review\n",
    "\n",
    "1. Learning Multimodal Transition Dynamics for Model-Based Reinforcement Learning [https://arxiv.org/abs/1705.00470v2](https://arxiv.org/abs/1705.00470v2) \n",
    "2. Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning [https://arxiv.org/abs/1708.02596v1](https://arxiv.org/abs/1708.02596v1) \n",
    "3. Neural Fitted Q Iteration - First Experiences with a Data Efficient Neural Reinforcement Learning Method [https://pdfs.semanticscholar.org/2820/01869bd502c7917db8b32b75593addfbbc68.pdf](https://pdfs.semanticscholar.org/2820/01869bd502c7917db8b32b75593addfbbc68.pdf) \n",
    "4. Deep Auto-Encoder Neural Networks in Reinforcement Learning [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.172.1873&rep=rep1&type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.172.1873&rep=rep1&type=pdf) \n",
    "5. Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images [http://papers.nips.cc/paper/5964-embed-to-control-a-locally-linear-latent-dynamics-model-for-control-from-raw-images.pdf](http://papers.nips.cc/paper/5964-embed-to-control-a-locally-linear-latent-dynamics-model-for-control-from-raw-images.pdf) \n",
    "6. A Deep Learning Approach for Joint Video Frame and Reward Prediction in Atari Games [https://arxiv.org/abs/1611.07078](https://arxiv.org/abs/1611.07078) \n",
    "7. Action-Conditional Video Prediction using Deep Networks in Atari Games [https://arxiv.org/abs/1507.08750](https://arxiv.org/abs/1507.08750) \n",
    "8. Imagination-Augmented Agents for Deep Reinforcement Learning [https://arxiv.org/abs/1707.06203](https://arxiv.org/abs/1707.06203) \n",
    "9. Model-Based Reinforcement Learning for Playing Atari Games [http://cs231n.stanford.edu/reports/2016/pdfs/116_Report.pdf](http://cs231n.stanford.edu/reports/2016/pdfs/116_Report.pdf) \n",
    "10. Learning model-based planning from scratch [https://arxiv.org/pdf/1708.02596v1.pdf](https://arxiv.org/pdf/1708.02596v1.pdf) \n",
    "11. Temporal Convolutional Policy Networks [https://bcourses.berkeley.edu/files/70257152/download](https://bcourses.berkeley.edu/files/70257152/download)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection\n",
    "Originally we thought that to train a good environment model, we should start collecting data with a pre-trained agent, then gradually degrade that agent's performance to random by introducing more and more random actions instead of what the agent suggests. In a real world task, you can imagine using human demonstrations to generate the world model instead.\n",
    "\n",
    "For Cartpole, it seems the state space is pretty well explored by completely random actions, so we never bothered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-19 09:47:31,707] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import gym\n",
    "from tqdm import tqdm_notebook\n",
    "from skimage.transform import resize\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2) Box(4,) (4,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83bc0239f394476d8370518172d45c31"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess(im):\n",
    "    return np.uint8(resize(np.mean(im, axis=-1), (im.shape[0]/4,im.shape[1]/4), mode='edge'))\n",
    "\n",
    "print(env.action_space, env.observation_space, env.observation_space.high.shape)\n",
    "frames, rewards, actions, observations = [], [], [], []\n",
    "n_frames = 0\n",
    "# CartPole ends at 200, but useful for other envs?\n",
    "MAX_FRAMES_PER_EPISODE = 1000\n",
    "# ~2GB of data for CartPole\n",
    "FRAMES_TO_COLLECT = 66000\n",
    "\n",
    "t = time.time()\n",
    "with tqdm_notebook(total=FRAMES_TO_COLLECT) as pbar:\n",
    "    while n_frames < FRAMES_TO_COLLECT:\n",
    "        observation = env.reset()\n",
    "        fs = []\n",
    "        rs = []\n",
    "        as_ = []\n",
    "        os = []\n",
    "        for _ in range(MAX_FRAMES_PER_EPISODE):\n",
    "            fs.append(preprocess(env.render(mode = 'rgb_array')))\n",
    "            action = env.action_space.sample()\n",
    "            observation, reward, done, _ = env.step(action)\n",
    "            as_.append(action)\n",
    "            rs.append(reward)\n",
    "            os.append(observation)\n",
    "            n_frames += 1\n",
    "            if done:\n",
    "                frames.append(fs)\n",
    "                rewards.append(rs)\n",
    "                actions.append(as_)\n",
    "                observations.append(os)\n",
    "                pbar.update(len(fs))\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data\n",
    "env_name = 'cartpole'\n",
    "np.save('%s_frames' % env_name, frames)\n",
    "np.save('%s_rewards' % env_name, rewards)\n",
    "np.save('%s_observations' % env_name, observations)\n",
    "np.save('%s_actions' % env_name, actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of stacked frames (66015, 100, 150)\n",
      "Range of values 0 to 255\n",
      "Bounding box (43, 78, 32, 120)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWwAAAD/CAYAAADVGuzgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhIEtSYX4/wT3uq4eBIDsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5d587b1160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "all_frames = np.vstack(np.stack(x) for x in frames)\n",
    "print('Shape of stacked frames', all_frames.shape)\n",
    "combined = np.min(all_frames, axis=0)\n",
    "plt.imshow(combined, cmap='gray', vmin=0, vmax=255)\n",
    "print('Range of values', np.min(combined), 'to', np.max(combined))\n",
    "def bbox(img):\n",
    "    '''Returns y_min, y_max, x_min, x_max\n",
    "    \n",
    "    https://stackoverflow.com/a/31402351/614529\n",
    "    '''\n",
    "    a = np.where(img != 255)\n",
    "    bounds = np.min(a[0]), np.max(a[0]), np.min(a[1]), np.max(a[1])\n",
    "    return bounds\n",
    "# This would be useful if we wanted to reduce input dimensionality \n",
    "# by cropping away the extra whitespace.\n",
    "print('Bounding box', bbox(combined))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the environment model\n",
    "Next, we use this collected data to train an autoencoder that takes a stack of frames and actions as inputs and outputs the next frame. This architecture requires no domain-specific information, though for games like Atari the framestack needs to be 4 frames deep to still pick up dynamics despite flickering.\n",
    "\n",
    "For a simple autoencoder we started with https://blog.keras.io/building-autoencoders-in-keras.html and modified it to our purposes. Key things to note:\n",
    "1. Dimensions matter! When scaling down and back up again, if the input dimensions don't evenly divide by the max pool layers or stride, when you scale back up, the dimensions won't match. E.g., 20//3 = 6, 6 * 3 = 18, 18 != 20. Two ways to get around this are\n",
    "  1. Scale your inputs to a number that's properly divisible (expensive)\n",
    "  1. Crop your inputs to a divisible number\n",
    "  1. Pad your inputs to get to a divisible number (best, since it doesn't require any domain knowledge)\n",
    "1. Convolutions don't work on framestacks, so we use a TimeDistributed layer wrapper to get around that. After everything gets flattened and fed into the dense layers, there's a chance for the environment dynamics to be processed. We could have put the frame stacks in the channel dimension but this seemed like a more general solution in case we wanted to use colors for some other environment.\n",
    "1. Initially we just concatenated the action (0 or 1 in CartPole) with the input to the bottleneck layer, but this didn't give the network enough flexibility to condition the outputs appropriately, nor would it scale to different number of actions. Instead, we one-hot encode the actions per standard practice and put a few Dense layers in front of them. We also tried tiling the actions per Deepmind's I2A, but it seemed like a waste of computation and had worse performance.\n",
    "1. The data is big enough that we have to get it in batches using `fit_generator`, else even my 64GB RAM machine OOM's.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 3, 96, 144, 1) 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistribu (None, 3, 96, 144, 16 160         input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistribu (None, 3, 48, 72, 16) 0           time_distributed_1[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistribu (None, 3, 48, 72, 16) 2320        time_distributed_2[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistribu (None, 3, 24, 36, 16) 0           time_distributed_3[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistribu (None, 3, 24, 36, 16) 2320        time_distributed_4[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 2)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistribu (None, 3, 12, 18, 16) 0           time_distributed_5[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 16)            48          input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 10368)         0           time_distributed_6[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 16)            272         dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 32)            331808      flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 16)            272         dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 48)            0           dense_1[0][0]                    \n",
      "                                                                   dense_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "bottleneck (Dense)               (None, 4)             196         concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 3456)          17280       bottleneck[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)              (None, 1, 12, 18, 16) 0           dense_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistribu (None, 1, 12, 18, 16) 2320        reshape_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_8 (TimeDistribu (None, 1, 24, 36, 16) 0           time_distributed_7[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_9 (TimeDistribu (None, 1, 24, 36, 16) 2320        time_distributed_8[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_10 (TimeDistrib (None, 1, 48, 72, 16) 0           time_distributed_9[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_11 (TimeDistrib (None, 1, 48, 72, 16) 2320        time_distributed_10[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_12 (TimeDistrib (None, 1, 96, 144, 16 0           time_distributed_11[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_13 (TimeDistrib (None, 1, 96, 144, 1) 145         time_distributed_12[0][0]        \n",
      "====================================================================================================\n",
      "Total params: 361,781\n",
      "Trainable params: 361,781\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Logging to Tensorboard in ./logs\n",
      "Epoch 1/10\n",
      "803/802 [==============================] - 40s - loss: 0.0010 - val_loss: 6.4683e-04\n",
      "Epoch 2/10\n",
      "803/802 [==============================] - 39s - loss: 5.7750e-04 - val_loss: 5.0931e-04\n",
      "Epoch 3/10\n",
      "803/802 [==============================] - 39s - loss: 5.2797e-04 - val_loss: 5.3990e-04\n",
      "Epoch 4/10\n",
      "803/802 [==============================] - 40s - loss: 5.0536e-04 - val_loss: 5.1645e-04\n",
      "Epoch 5/10\n",
      "803/802 [==============================] - 40s - loss: 4.8687e-04 - val_loss: 4.6629e-04\n",
      "Epoch 6/10\n",
      "803/802 [==============================] - 40s - loss: 4.6894e-04 - val_loss: 4.5659e-04\n",
      "Epoch 7/10\n",
      "803/802 [==============================] - 40s - loss: 3.7768e-04 - val_loss: 3.2453e-04\n",
      "Epoch 8/10\n",
      "803/802 [==============================] - 40s - loss: 2.3268e-04 - val_loss: 2.6458e-04\n",
      "Epoch 9/10\n",
      "803/802 [==============================] - 40s - loss: 2.0941e-04 - val_loss: 1.6796e-04\n",
      "Epoch 10/10\n",
      "803/802 [==============================] - 40s - loss: 1.9553e-04 - val_loss: 1.8247e-04\n"
     ]
    }
   ],
   "source": [
    "from autoencoder import load_data, make_model, train\n",
    "_, windowed_frames, windowed_frames_next, windowed_actions = load_data(window=3)\n",
    "model = make_model(windowed_frames)\n",
    "train(model, windowed_frames, windowed_frames_next, windowed_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
