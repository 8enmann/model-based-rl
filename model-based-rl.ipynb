{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-based reinforcement learning\n",
    "Ben Mann and Andrew Schreiber\n",
    "### Motivation\n",
    "\n",
    "Model-based learning \n",
    "\n",
    "- may be more introspectable than model-free learning. \n",
    "- is outperforming model-free learning at irreversible games (see [Sokoban imagination](https://arxiv.org/pdf/1707.06203.pdf)). It hasn't successfully been applied to Atari. \n",
    "- algorithms can be informed that part of the training space is untouchable (i.e. donâ€™t simulate human minds being tortured, we can tell you ahead of time that is bad). \n",
    "\n",
    "### Summary of idea\n",
    "\n",
    "DQN learns on CartPole much faster when input is [velocity, position, angle, angular velocity] vs pixels.\n",
    "\n",
    "Can we use an unsupervised generative model to collapse a high dimensional state representation to speed up model-free learning? Evaluate on classic control problems and basic Atari games.\n",
    "\n",
    "### Related work\n",
    "\n",
    "[5] uses a deep VAE to learn a low-dimensional representation of classic control problems, but doesn't use reinforcement learning on top of it.\n",
    "\n",
    "[7] and [9] build on Atari next video frame prediction [6] but fail to beat DQN performance.\n",
    "\n",
    "[8] merges model-based and model-free techniques but doesn't report Atari success.\n",
    "\n",
    "[2] uses the idea of combining model-based and model-free techniques for the purpose of data efficiency, but it doesn't operate on pixels, and in mujoco state + action should perfectly predict next state unlike atari where other agents can do stuff\n",
    "\n",
    "  \n",
    "\n",
    "So the big difference here is that we're aiming to beat DQN at data efficiency on Atari using an approximation of f(s, a) -&gt; s'. We start with CartPole to validate the approach and move on to Pong.\n",
    "\n",
    "### Project Roadmap / ideas\n",
    "\n",
    "1. [Cartpole](https://arxiv.org/pdf/1602.01783v1.pdf), [MountainCar](https://gym.openai.com/envs/MountainCar-v0), [Pendulum](https://gym.openai.com/envs/Pendulum-v0) \n",
    "    1. A2C on dense representation learns fast (upper bound of learning speed). Compare to \n",
    "        1. A2C trained on autoencoder-learned dense representation \n",
    "        2. A2C trained on pixels directly \n",
    "\n",
    "2. Pong \n",
    "    1. Is a dense representation learned from full-resolution color better than from downsample? \n",
    "        1. How much does model capacity matter as input resolution varies? \n",
    "\n",
    "    2. Can we learn faster than from pixels? \n",
    "    3. [This Stanford group failed to beat DQN](http://cs231n.stanford.edu/reports/2016/pdfs/116_Report.pdf) [9]. They say it's because compounding errors made long-term prediction hard, and pong and breakout have very long (>50) timestep dependencies. Seems like we could get around this by \n",
    "        1. predicting N steps ahead in one forward pass and\n",
    "        2. feed all N steps into model a la I2A [8] so that later frames can be discarded dynamically.\n",
    "    4. Can we do long-range, cheap rollouts using [dilated convolutions](http://www.inference.vc/dilated-convolutions-and-kronecker-factorisation/) [11]? Sokoban noted even 5-step rollouts were quite expensive \n",
    "\n",
    "3. Breakout(?) \n",
    "    1. Might be easier than pong due to denser rewards \n",
    "\n",
    "4. Montezuma (stretch goal) \n",
    "    1. Try adding noise to the input image, train an Inspector \n",
    "    2. Detect abnormally high variance in autoencoder's decoded output frame, use that as a signal to update model \n",
    "    3. Train new autoencoder per room, auto-detect when out of distribution a la \"Modular Multitask Reinforcement Learning with Policy Sketches\" [https://arxiv.org/abs/1611.01796v2](https://arxiv.org/abs/1611.01796v2) \n",
    "    4. Use an ensemble of autoencoders so that any of them can be swapped out or modified and agent still performs \n",
    "\n",
    "### Concrete steps\n",
    "\n",
    "1. Run env (CartPole-v0, Pendulum, MountainCar) \n",
    "2. Generate training data for autoencoder model using pre-trained DQN / random agent interpolation to maximize search space \n",
    "3. Train autoencoder to repro -- a/b split here \n",
    "    1. a frame \n",
    "    2. Sequence of frames \n",
    "\n",
    "4. Use dense autoencoder representation to preprocess frames and train a model \n",
    "5. Compare to upper and lower bound baselines (A2C trained on \"classic control\" signals and A2C trained on pixels) \n",
    "\n",
    "### Additional questions\n",
    "\n",
    "1. Does the learned autoencoder representation correlate with the real 4 dimensional cartpole observation? Chart r^2 for all combinations of variables. Chart scatter x vs y. \n",
    "2. For a random input state, what is the delta in the decoded representation given one action vs the other? If it's small, can we add a term to the loss to make it larger? \n",
    "3. How do loss and computational cost differ given different initial downsampling? Can we get away with preserving more detail? Does using 2 frame stack instead of 4 help? \n",
    "4. How quickly do imagination rollouts drift? How bad do they look? \n",
    "5. Would hard attention be an effective dimensionality reducer since most of the space is blank? \n",
    "6. How much better is VAE vs vanilla AE? \n",
    "  \n",
    "\n",
    "### Literature review\n",
    "\n",
    "1. Learning Multimodal Transition Dynamics for Model-Based Reinforcement Learning [https://arxiv.org/abs/1705.00470v2](https://arxiv.org/abs/1705.00470v2) \n",
    "2. Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning [https://arxiv.org/abs/1708.02596v1](https://arxiv.org/abs/1708.02596v1) \n",
    "3. Neural Fitted Q Iteration - First Experiences with a Data Efficient Neural Reinforcement Learning Method [https://pdfs.semanticscholar.org/2820/01869bd502c7917db8b32b75593addfbbc68.pdf](https://pdfs.semanticscholar.org/2820/01869bd502c7917db8b32b75593addfbbc68.pdf) \n",
    "4. Deep Auto-Encoder Neural Networks in Reinforcement Learning [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.172.1873&rep=rep1&type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.172.1873&rep=rep1&type=pdf) \n",
    "5. Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images [http://papers.nips.cc/paper/5964-embed-to-control-a-locally-linear-latent-dynamics-model-for-control-from-raw-images.pdf](http://papers.nips.cc/paper/5964-embed-to-control-a-locally-linear-latent-dynamics-model-for-control-from-raw-images.pdf) \n",
    "6. A Deep Learning Approach for Joint Video Frame and Reward Prediction in Atari Games [https://arxiv.org/abs/1611.07078](https://arxiv.org/abs/1611.07078) \n",
    "7. Action-Conditional Video Prediction using Deep Networks in Atari Games [https://arxiv.org/abs/1507.08750](https://arxiv.org/abs/1507.08750) \n",
    "8. Imagination-Augmented Agents for Deep Reinforcement Learning [https://arxiv.org/abs/1707.06203](https://arxiv.org/abs/1707.06203) \n",
    "9. Model-Based Reinforcement Learning for Playing Atari Games [http://cs231n.stanford.edu/reports/2016/pdfs/116_Report.pdf](http://cs231n.stanford.edu/reports/2016/pdfs/116_Report.pdf) \n",
    "10. Learning model-based planning from scratch [https://arxiv.org/pdf/1708.02596v1.pdf](https://arxiv.org/pdf/1708.02596v1.pdf) \n",
    "11. Temporal Convolutional Policy Networks [https://bcourses.berkeley.edu/files/70257152/download](https://bcourses.berkeley.edu/files/70257152/download)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection\n",
    "Originally we thought that to train a good environment model, we should start collecting data with a pre-trained agent, then gradually degrade that agent's performance to random by introducing more and more random actions instead of what the agent suggests. In a real world task, you can imagine using human demonstrations to generate the world model instead.\n",
    "\n",
    "For Cartpole, it seems the state space is pretty well explored by completely random actions, so we never bothered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-19 16:30:00,144] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import gym\n",
    "from tqdm import tqdm_notebook\n",
    "from skimage.transform import resize\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2) Box(4,) (4,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3a8bc2812324c5c99c1f792b907e879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=66000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def downsample(im):\n",
    "    return np.uint8(resize(np.mean(im, axis=-1), (im.shape[0]/4,im.shape[1]/4), mode='edge'))\n",
    "\n",
    "print(env.action_space, env.observation_space, env.observation_space.high.shape)\n",
    "frames, rewards, actions, observations = [], [], [], []\n",
    "n_frames = 0\n",
    "# CartPole ends at 200, but useful for other envs?\n",
    "MAX_FRAMES_PER_EPISODE = 1000\n",
    "# ~2GB of data for CartPole\n",
    "FRAMES_TO_COLLECT = 66000\n",
    "\n",
    "t = time.time()\n",
    "with tqdm_notebook(total=FRAMES_TO_COLLECT) as pbar:\n",
    "    while n_frames < FRAMES_TO_COLLECT:\n",
    "        observation = env.reset()\n",
    "        fs = []\n",
    "        rs = []\n",
    "        as_ = []\n",
    "        os = []\n",
    "        for _ in range(MAX_FRAMES_PER_EPISODE):\n",
    "            fs.append(downsample(env.render(mode = 'rgb_array')))\n",
    "            action = env.action_space.sample()\n",
    "            observation, reward, done, _ = env.step(action)\n",
    "            as_.append(action)\n",
    "            rs.append(reward)\n",
    "            os.append(observation)\n",
    "            n_frames += 1\n",
    "            if done:\n",
    "                frames.append(fs)\n",
    "                rewards.append(rs)\n",
    "                actions.append(as_)\n",
    "                observations.append(os)\n",
    "                pbar.update(len(fs))\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data\n",
    "env_name = 'cartpole'\n",
    "np.save('%s_frames' % env_name, frames)\n",
    "np.save('%s_rewards' % env_name, rewards)\n",
    "np.save('%s_observations' % env_name, observations)\n",
    "np.save('%s_actions' % env_name, actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "all_frames = np.vstack(np.stack(x) for x in frames)\n",
    "print('Shape of stacked frames', all_frames.shape)\n",
    "combined = np.min(all_frames, axis=0)\n",
    "plt.imshow(combined, cmap='gray', vmin=0, vmax=255)\n",
    "print('Range of values', np.min(combined), 'to', np.max(combined))\n",
    "def bbox(img):\n",
    "    '''Returns y_min, y_max, x_min, x_max\n",
    "    \n",
    "    https://stackoverflow.com/a/31402351/614529\n",
    "    '''\n",
    "    a = np.where(img != 255)\n",
    "    bounds = np.min(a[0]), np.max(a[0]), np.min(a[1]), np.max(a[1])\n",
    "    return bounds\n",
    "# This would be useful if we wanted to reduce input dimensionality \n",
    "# by cropping away the extra whitespace.\n",
    "print('Bounding box', bbox(combined))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the environment model\n",
    "Next, we use this collected data to train an autoencoder that takes a stack of frames and actions as inputs and outputs the next frame. This architecture requires no domain-specific information, though for games like Atari the framestack needs to be 4 frames deep to still pick up dynamics despite flickering.\n",
    "\n",
    "For a simple autoencoder we started with https://blog.keras.io/building-autoencoders-in-keras.html and modified it to our purposes. Key things to note:\n",
    "1. Dimensions matter! When scaling down and back up again, if the input dimensions don't evenly divide by the max pool layers or stride, when you scale back up, the dimensions won't match. E.g., 20//3 = 6, 6 * 3 = 18, 18 != 20. Two ways to get around this are\n",
    "  1. Scale your inputs to a number that's properly divisible (expensive)\n",
    "  1. Crop your inputs to a divisible number\n",
    "  1. Pad your inputs to get to a divisible number (best, since it doesn't require any domain knowledge)\n",
    "1. Convolutions don't work on framestacks, so we use a TimeDistributed layer wrapper to get around that. After everything gets flattened and fed into the dense layers, there's a chance for the environment dynamics to be processed. We could have put the frame stacks in the channel dimension but this seemed like a more general solution in case we wanted to use colors for some other environment.\n",
    "1. Initially we just concatenated the action (0 or 1 in CartPole) with the input to the bottleneck layer, but this didn't give the network enough flexibility to condition the outputs appropriately, nor would it scale to different number of actions. Instead, we one-hot encode the actions per standard practice and put a few Dense layers in front of them. We also tried tiling the actions per Deepmind's I2A, but it seemed like a waste of computation and had worse performance.\n",
    "1. The data is big enough that we have to get it in batches using `fit_generator`, else even my 64GB RAM machine OOM's.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See autoencoder.py for the meat here.\n",
    "from autoencoder import load_data, make_model, train\n",
    "_, windowed_frames, windowed_frames_next, windowed_actions = load_data(window=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = make_model(windowed_frames)\n",
    "train(model, windowed_frames, windowed_frames_next, windowed_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from disk in case we want to start from here without training.\n",
    "from keras.models import load_model, Model\n",
    "model = load_model('autoencoder.h5')\n",
    "encoder_model = Model(model.input, model.get_layer('bottleneck').output, name='encoder')\n",
    "print(model.input_shape, model.output_shape, encoder_model.output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model exploration\n",
    "Let's verify that the model picked up the properties we expect.\n",
    "1. For a random input state, what is the delta in the decoded representation given one action vs the other? If it's small, can we add a term to the loss to make it larger?\n",
    "1. Does the learned autoencoder representation correlate with the real 4 dimensional cartpole observation? Chart r2 for all combinations of variables. Chart scatter x vs y.\n",
    "1. How quickly do imagination rollouts drift? How bad do they look?\n",
    "\n",
    "When I initially ran this data exploration, the scatter chart looked pretty bad and the imagination rollouts were nearly the same regardless of the input actions. After a lot of experimentation, it got much better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW = 3\n",
    "def one_hot(x, n_classes):\n",
    "    size = x.shape[0]\n",
    "    one_hot = np.zeros((size, n_classes))\n",
    "    one_hot[range(size), x] = 1\n",
    "    return one_hot\n",
    "\n",
    "one_hot_actions = one_hot(windowed_actions, 2)\n",
    "print(one_hot_actions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left vs right encoded representation\n",
    "First let's see if the encoded representation looks at all similar to the actual CartPole data (position, velocity, angle, angular velocity). You can see the teal line on the left roughly tracks the green line on the right. Similarly, the teal line on the right roughly tracks the red line on the left. The delta between the two charts on the right is hard to see, so take a look at the bottom left chart. Values range between 0 and .6, which seems reasonable given the SELU nonlinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_FRAMES = 100\n",
    "from autoencoder import preprocess, eps_to_stacked_window\n",
    "\n",
    "windowed_observations = eps_to_stacked_window(observations, window=WINDOW, offset=True)\n",
    "left_pred = encoder_model.predict([preprocess(windowed_frames[:N_FRAMES]), one_hot(np.zeros(N_FRAMES, dtype=np.uint8), 2)])\n",
    "right_pred = encoder_model.predict([preprocess(windowed_frames[:N_FRAMES]), one_hot(np.ones(N_FRAMES, dtype=np.uint8), 2)])\n",
    "pred = encoder_model.predict([preprocess(windowed_frames[:N_FRAMES]), one_hot_actions[:N_FRAMES]])\n",
    "\n",
    "plt.figure(figsize=(18, 12))\n",
    "plt.subplot(2,2,1)\n",
    "plt.title('Classic cartpole observation of next frame')\n",
    "plt.plot(windowed_observations[:N_FRAMES, WINDOW-1, :])\n",
    "plt.subplot(2,2,2)\n",
    "plt.title('Encoded representation for action taken')\n",
    "plt.plot(pred)\n",
    "plt.subplot(2,2,4)\n",
    "plt.plot(left_pred)\n",
    "plt.title('Encoded representation for LEFT action')\n",
    "plt.subplot(2,2,3)\n",
    "plt.title('Delta between left and right actions')\n",
    "plt.plot(left_pred - right_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation with actual CartPole dynamics\n",
    "Next, we hope that at least one of the encoded channels picked up each of the real variables, even though it doesn't know anything about them. What we're looking for is a correlation coefficient close to 1 or -1 in every row. We can see that the position row has 0.99, the angle row has .93, the velocity row has .73, and the angular velocity row has .77. So the encoder had a harder time learning the dynamic channels than the static ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 12))\n",
    "row_labels = ['position', 'velocity', 'angle', 'angular\\nvelocity']\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        ax = plt.subplot(4,4,i * 4 + j + 1)\n",
    "        x,y = windowed_observations[:N_FRAMES, WINDOW-1, i], pred[:N_FRAMES, j]\n",
    "        if j == 0:\n",
    "            ax.set_ylabel(row_labels[i], rotation=0, labelpad=30, size='large')\n",
    "        plt.title('r: %.2f' % np.corrcoef(x,y)[0,1])\n",
    "        plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imagination\n",
    "Our model takes a stack of frames and an action and outputs the predicted next frame. An imagination rollout takes an initial condition and feeds the model with its own predictions for N steps (15 in this case). \n",
    "\n",
    "We're looking for the cart to move left when we give it a stream of left actions, and right when we give it a stream of right actions. When I first ran this, it moved left no matter what actions it was given. You can see that one moves slower because initially the pole was tipping the other way. Seems reasonable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from array2gif import write_gif\n",
    "from IPython.display import Image\n",
    "\n",
    "def unprocess(frames):\n",
    "    '''Invert, scale, remove extra dims'''\n",
    "    return (1 - np.squeeze(frames)) * 255\n",
    "\n",
    "def to_rgb(im, scale=None):\n",
    "    '''Convert a grayscale image into the format array2gif expects.'''\n",
    "    if scale:\n",
    "        im = np.stack(resize(x, (im.shape[1] * scale, im.shape[2] * scale), mode='edge') for x in im)\n",
    "    return [np.stack((x,) * 3).astype(np.uint8) for x in im]\n",
    "        \n",
    "def rollout(action):\n",
    "    frames = list(preprocess(windowed_frames[0]))\n",
    "    # How many steps to rollout\n",
    "    N_STEPS = 15\n",
    "    for _ in range(N_STEPS):\n",
    "        x = np.expand_dims(np.array(frames[-WINDOW:]), axis=0)\n",
    "        pred = model.predict([x, np.array(action)])\n",
    "        # Put the extra dimension on the end\n",
    "        frames.append(np.expand_dims(np.squeeze(pred), axis=-1))\n",
    "    return np.array(frames)\n",
    "\n",
    "def display(frames, name):\n",
    "    write_gif(to_rgb(frames, scale=2), filename=name, fps=20)\n",
    "    return Image(filename=name)\n",
    "\n",
    "left = rollout([[0,1]])\n",
    "right = rollout([[1,0]])\n",
    "display(unprocess(np.concatenate([left, right], axis=1)), 'test.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "079b7143aa7d4904a465ce59ed9572c3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "0eb9124a2cad4f6b890bc357c490bc95": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "13793ee602194d9bbec735d17e20c023": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.0.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "danger",
       "layout": "IPY_MODEL_32d6bca9df084c8794b552ecb1dbcfbf",
       "max": 66000,
       "style": "IPY_MODEL_924f9789009e424890c2eeae4894c692",
       "value": 8272
      }
     },
     "210bc7fee4cb41f8841c24c544feb3ed": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_13793ee602194d9bbec735d17e20c023",
        "IPY_MODEL_67e8c185f44d48b5abd304ce6897e835"
       ],
       "layout": "IPY_MODEL_add7f99e1e164fcc8e91b233d2d399ec"
      }
     },
     "22ca0168112540659cd5a55223445f09": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "32d6bca9df084c8794b552ecb1dbcfbf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "34c4f720efe643d991f9def74afc5264": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "3e427882f4d843e49e799e3f6baa284c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.0.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "danger",
       "layout": "IPY_MODEL_ff1c03800f6d4c26adbf97a1b18d38bb",
       "max": 66000,
       "style": "IPY_MODEL_7483c0ff9b1b4a44a059ad5c5ba787f5",
       "value": 3085
      }
     },
     "446092791ec94a00b9f8b50dddcda47c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "4884f095d79d41fcbfdb161e316091e0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_6afb36519db541f2bf29807b03626c37",
       "style": "IPY_MODEL_446092791ec94a00b9f8b50dddcda47c",
       "value": " 45% 29644/66000 [04:22&lt;05:14, 115.76it/s]"
      }
     },
     "672d759a91564dbe8a41e0b2dd32f268": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "67e8c185f44d48b5abd304ce6897e835": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_6fb9ffe76c3a4b31ad9d8b8acb4c03b2",
       "style": "IPY_MODEL_912831e290804ef498b5e48df301ab2a",
       "value": " 13% 8286/66000 [01:13&lt;08:31, 112.76it/s]"
      }
     },
     "6afb36519db541f2bf29807b03626c37": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6fb9ffe76c3a4b31ad9d8b8acb4c03b2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7483c0ff9b1b4a44a059ad5c5ba787f5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "8151e2a835e24cd4820b8cf34b9fc835": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "86e752752d31417ba15f40f00a3d09d9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_cb9bac0bfd844de9b29615a74ebb596c",
       "style": "IPY_MODEL_f7ebbbe20fd1405f8a40eb716288dafb",
       "value": " 12% 8136/66000 [01:09&lt;08:12, 117.59it/s]"
      }
     },
     "8a51872dd5964232b8c32df47a0fc17c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.0.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "danger",
       "layout": "IPY_MODEL_aaebedb796df4cdba52d197b1e4a7a85",
       "max": 66000,
       "style": "IPY_MODEL_672d759a91564dbe8a41e0b2dd32f268",
       "value": 29644
      }
     },
     "8ea610b6ef3b4aed946e464628416b48": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "912831e290804ef498b5e48df301ab2a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "924f9789009e424890c2eeae4894c692": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "a3a8bc2812324c5c99c1f792b907e879": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c4e27a7fa9aa45a28ab1288af7c22bed",
        "IPY_MODEL_86e752752d31417ba15f40f00a3d09d9"
       ],
       "layout": "IPY_MODEL_22ca0168112540659cd5a55223445f09"
      }
     },
     "aa689f466179489caf1e0d2e49fab18e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_8a51872dd5964232b8c32df47a0fc17c",
        "IPY_MODEL_4884f095d79d41fcbfdb161e316091e0"
       ],
       "layout": "IPY_MODEL_0eb9124a2cad4f6b890bc357c490bc95"
      }
     },
     "aaebedb796df4cdba52d197b1e4a7a85": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "add7f99e1e164fcc8e91b233d2d399ec": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bb95059acbd84e039425ff944e42ea88": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c4e27a7fa9aa45a28ab1288af7c22bed": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.0.0",
      "model_name": "IntProgressModel",
      "state": {
       "layout": "IPY_MODEL_8ea610b6ef3b4aed946e464628416b48",
       "max": 66000,
       "style": "IPY_MODEL_8151e2a835e24cd4820b8cf34b9fc835",
       "value": 8136
      }
     },
     "cb9bac0bfd844de9b29615a74ebb596c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d83d00bece934d41badf2ea1229299c9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_3e427882f4d843e49e799e3f6baa284c",
        "IPY_MODEL_f8c00f28825348cf85ea02e826ce4ce8"
       ],
       "layout": "IPY_MODEL_bb95059acbd84e039425ff944e42ea88"
      }
     },
     "f7ebbbe20fd1405f8a40eb716288dafb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "f8c00f28825348cf85ea02e826ce4ce8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_079b7143aa7d4904a465ce59ed9572c3",
       "style": "IPY_MODEL_34c4f720efe643d991f9def74afc5264",
       "value": "  5% 3085/66000 [00:27&lt;08:59, 116.62it/s]"
      }
     },
     "ff1c03800f6d4c26adbf97a1b18d38bb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
